<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8" />
<meta name="viewport" content="width=device-width,initial-scale=1" />
<title>4× USB Webcam Viewer (Master-Sync, mtime-based length)</title>
<style>
  body { margin:0; font-family:system-ui,Segoe UI,Roboto,Arial,sans-serif; background:#111; color:#eee; display:flex; height:100vh; overflow:hidden; }
  #sidebar { width:260px; background:#1b1b1b; border-right:1px solid #333; display:flex; flex-direction:column; }
  #sessionList { flex:1; overflow-y:auto; padding:8px; }
  .session { padding:8px 10px; border-radius:6px; margin-bottom:4px; background:#222; cursor:pointer; }
  .session:hover { background:#2c2c2c; }
  .session.active { background:#2c7be5; color:#fff; }
  #main { flex:1; display:flex; flex-direction:column; overflow:hidden; }
  #toolbar { padding:8px 12px; border-bottom:1px solid #333; background:#141414; display:flex; align-items:center; gap:8px; }
  #videos { flex:1; display:grid; grid-template-columns:repeat(2,1fr); grid-template-rows:repeat(2,1fr); gap:6px; padding:6px; }
  video { width:100%; height:100%; object-fit:contain; background:#000; }
  .vwrap { position:relative; }
  .vwrap header { position:absolute; top:4px; left:4px; background:rgba(0,0,0,0.5); padding:2px 6px; border-radius:4px; font-size:12px; }
  /* === Detection additions: overlay canvas always stretches to video box === */
  .overlay {
    position:absolute; inset:0; pointer-events:none;
  }
  #timeline { width:100%; }
  #currentTime { width:70px; text-align:right; font-variant-numeric:tabular-nums; }
  #sessionName { font-variant-numeric:tabular-nums; }
  button { background:#2c7be5; border:none; color:#fff; padding:6px 10px; border-radius:6px; cursor:pointer; }
  button:hover { background:#3d8af0; }
  label { display:flex; align-items:center; gap:6px; }
  input[type="number"] { width:76px; }
</style>

<!-- === Detection additions: TFJS + COCO-SSD === -->
<script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@4.21.0/dist/tf.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/coco-ssd@2.2.3/dist/coco-ssd.min.js"></script>
</head>
<body>
  <div id="sidebar">
    <div style="padding:8px;border-bottom:1px solid #333;">
      <button id="pickFolder">Open Folder</button>
    </div>
    <div id="sessionList"></div>
  </div>

  <div id="main">
    <div id="toolbar">
      <button id="playPause" disabled>▶️ Play</button>
      <input id="timeline" type="range" min="0" max="100" value="0" step="0.01" disabled />
      <span id="currentTime">00:00</span>
      <div style="flex:1"></div>
      <!-- === Detection additions: controls on the right === -->
      <label title="Run object detection on the Front camera">
        <input type="checkbox" id="detectToggle" />
        Detect Front Objects
      </label>
      <label title="Horizontal FOV of the Front camera (degrees)">
        FOV°
        <input type="number" id="fovInput" min="20" max="180" step="0.1" value="90" />
      </label>
      <span id="sessionName"></span>
    </div>

    <div id="videos">
      <div class="vwrap">
        <header>Front</header>
        <video id="vidFront" preload="metadata" muted></video>
        <!-- === Detection additions: overlay canvas === -->
        <canvas id="ovlFront" class="overlay"></canvas>
      </div>
      <div class="vwrap"><header>Back</header> <video id="vidBack"  preload="metadata" muted></video></div>
      <div class="vwrap"><header>Left</header> <video id="vidLeft"  preload="metadata" muted></video></div>
      <div class="vwrap"><header>Right</header><video id="vidRight" preload="metadata" muted></video></div>
    </div>
  </div>

<script>
(async () => {
  const pickBtn = document.getElementById('pickFolder');
  const sessionList = document.getElementById('sessionList');
  const playPause = document.getElementById('playPause');
  const timeline = document.getElementById('timeline');
  const currentTime = document.getElementById('currentTime');
  const sessionNameEl = document.getElementById('sessionName');
  const vids = {
    Front: document.getElementById('vidFront'),
    Back:  document.getElementById('vidBack'),
    Left:  document.getElementById('vidLeft'),
    Right: document.getElementById('vidRight')
  };

  // === Detection additions ===
  const ovlFront = document.getElementById('ovlFront');
  const detectToggle = document.getElementById('detectToggle');
  const fovInput = document.getElementById('fovInput');

  // Classes to keep, mapping TF label -> display label and assumed height (m)
  const CLASS_HEIGHTS = {
    person: 1.70,
    car: 1.50,
    bus: 3.20,
    truck: 3.00,
    bicycle: 1.50,
    motorcycle: 1.40
  };
  const KEEP = new Set(Object.keys(CLASS_HEIGHTS));
  let cocoModel = null;          // loaded coco-ssd model
  let detRunning = false;        // whether the rVFC detection loop is active
  let cancelDetCb = null;        // cancel handle for rVFC
  let lastInferT = 0;            // simple throttle
  const INFER_EVERY_MS = 120;    // ~8 FPS infer cap (adjust as needed)

  let sessions = {};     // { "YYYYMMDD_HHMMSS": {Front:File,Back:File,Left:File,Right:File} }
  let currentSession = null;
  let baseDate = new Date(NaN);  // full date from filename (local)
  let master = null;             // clock video
  let rVFCHandle = null;         // rVFC handle
  let playing = false;

  // Sync thresholds (seconds)
  const SOFT_SLEW_TH = 0.030;    // start gentle playbackRate correction above 30 ms
  const HARD_JUMP_TH = 0.100;    // jump if drift exceeds 100 ms
  const MAX_SLEW = 0.03;         // cap playbackRate adjustment to ±3%

  // ---------- Utils ----------
  function fmtTime(sec){
    sec = Math.max(0,sec);
    const m=Math.floor(sec/60), s=Math.floor(sec%60);
    return `${String(m).padStart(2,'0')}:${String(s).padStart(2,'0')}`;
  }
  function fmtHMSfromDate(dt){
    const h=dt.getHours(), m=dt.getMinutes(), s=dt.getSeconds();
    return `${String(h).padStart(2,'0')}:${String(m).padStart(2,'0')}:${String(s).padStart(2,'0')}`;
  }
  function parseBaseDate(stamp){
    // stamp like YYYYMMDD_HHMMSS
    const m = stamp.match(/^(\d{4})(\d{2})(\d{2})_(\d{2})(\d{2})(\d{2})$/);
    if(!m) return new Date(NaN);
    const [_, Y, Mo, D, h, mi, s] = m;
    return new Date(+Y, +Mo-1, +D, +h, +mi, +s, 0); // local
  }
  // Clamp + nice rounding
  const clamp = (v,min,max)=>Math.max(min,Math.min(max,v));
  const round1 = (x)=>Math.round(x*10)/10;

  async function pickFolder() {
    let files = [];
    if (window.showDirectoryPicker) {
      const dir = await window.showDirectoryPicker();
      for await (const entry of dir.values()) {
        if (entry.kind==='file' && entry.name.endsWith('.webm')) {
          files.push(await entry.getFile());
        }
      }
    } else {
      const input = document.createElement('input');
      input.type='file';
      input.multiple=true;
      input.accept='.webm';
      input.click();
      files = await new Promise(res=>{
        input.onchange=()=>res([...input.files]);
      });
    }
    parseSessions(files);
  }

  function parseSessions(files){
    sessions={};
    for(const f of files){
      // seconds-aware: YYYYMMDD_HHMMSS_Position.webm
      const m = f.name.match(/^(\d{8}_\d{6})_(Front|Back|Left|Right)\.webm$/i);
      if(!m) continue;
      const [,stamp,pos] = m;
      sessions[stamp] ||= {};
      sessions[stamp][pos] = f;
    }
    renderSidebar();
  }

  function renderSidebar(){
    sessionList.innerHTML='';
    const stamps = Object.keys(sessions).sort().reverse();
    for(const stamp of stamps){
      const div=document.createElement('div');
      div.className='session';
      div.textContent=stamp;
      div.onclick=()=>loadSession(stamp,div);
      sessionList.appendChild(div);
    }
  }

  async function loadSession(stamp,elem){
    // Stop previous playback and detection loops if any
    stopSyncLoop();
    stopDetectionLoop();
    playing = false;
    playPause.textContent='▶️ Play';
    playPause.dataset.state='paused';

    [...sessionList.children].forEach(c=>c.classList.remove('active'));
    elem.classList.add('active');
    currentSession=stamp;

    // Parse base date from filename and show initial global time
    baseDate = parseBaseDate(stamp);
    sessionNameEl.textContent = fmtHMSfromDate(baseDate);

    playPause.disabled=false;
    timeline.disabled=false;

    // Load videos; pick first available as master
    master = null;
    for(const pos of ['Front','Back','Left','Right']){
      const v = vids[pos];
      const file = sessions[stamp][pos];
      if(file){
        v.src = URL.createObjectURL(file);
        v.play().catch(()=>{});
        v.pause();
        v.currentTime=0;
        if (!master) master = v;
      } else {
        v.removeAttribute('src');
      }
      v.playbackRate = 1.0;
    }

    // Wait for metadata (fallback timeout)
    await Promise.all(Object.values(vids).map(v=> new Promise(r=>{
      if (!v.src) return r();
      v.onloadedmetadata=()=>r();
      setTimeout(r,500);
    })));

    // Resize overlay to match displayed box (CSS contains video, so use client size)
    resizeOverlayToFront();

    // Compute timeline max from mtime or duration
    const filesInSession = ['Front','Back','Left','Right']
      .map(pos => sessions[stamp][pos])
      .filter(Boolean);
    let elapsedByMtimeSec = 0;
    if (filesInSession.length) {
      const latestMs = Math.max(...filesInSession.map(f => f.lastModified || 0));
      const startMs  = baseDate.getTime();
      if (isFinite(latestMs) && isFinite(startMs) && latestMs > startMs) {
        elapsedByMtimeSec = Math.floor((latestMs - startMs) / 1000);
      }
    }
    const durations = Object.values(vids).map(v => (v.src ? (v.duration || 0) : 0));
    const maxDur = Math.max(...durations, 0);
    const totalSec = elapsedByMtimeSec > 0 ? elapsedByMtimeSec : maxDur;

    timeline.max = totalSec || 0;
    timeline.value=0;
    currentTime.textContent = fmtTime(0);
    sessionNameEl.textContent = fmtHMSfromDate(baseDate); // global time at t=0
  }

  // --- playback control ---
  playPause.addEventListener('click', async ()=>{
    if(!currentSession || !master) return;
    const isPlaying = playPause.dataset.state==='playing';
    if(isPlaying){
      pauseAll();
      stopDetectionLoop();
    } else {
      playAll();
      if (detectToggle.checked) {
        await ensureModelLoaded();
        startDetectionLoop();
      }
    }
  });

  function playAll(){
    playPause.textContent='⏸ Pause';
    playPause.dataset.state='playing';
    playing = true;

    for (const v of Object.values(vids)) {
      if (!v.src) continue;
      v.play().catch(()=>{});
    }
    startSyncLoop();
  }

  function pauseAll(){
    playPause.textContent='▶️ Play';
    playPause.dataset.state='paused';
    playing = false;

    for (const v of Object.values(vids)) {
      if (!v.src) continue;
      v.pause();
      v.playbackRate = 1.0;
    }
    stopSyncLoop();
  }

  // --- detection toggle ---
  detectToggle.addEventListener('change', async ()=>{
    if (detectToggle.checked && playing) {
      await ensureModelLoaded();
      startDetectionLoop();
    } else {
      stopDetectionLoop(true);
    }
  });

  // --- timeline scrubbing ---
  timeline.addEventListener('input', ()=>{
    if(!master) return;
    const t=parseFloat(timeline.value);
    master.currentTime = t;
    for (const v of Object.values(vids)) {
      if (v !== master && v.src) v.currentTime = t;
    }
    currentTime.textContent = fmtTime(t);
    const nowDt = new Date(baseDate.getTime() + Math.floor(t)*1000);
    sessionNameEl.textContent = fmtHMSfromDate(nowDt);
    // On scrub, also refresh overlay once (use last detection results? here we clear)
    clearOverlay();
  });

  // --- master-driven sync loop with rVFC + slew control ---
  function startSyncLoop(){
    if (!master) return;
    const followers = Object.values(vids).filter(v => v !== master && v.src);

    const loop = () => {
      const t = master.currentTime || 0;

      // clamp UI timeline to declared max (mtime-based)
      const tClamped = Math.min(t, parseFloat(timeline.max) || t);
      timeline.value = tClamped;
      currentTime.textContent = fmtTime(tClamped);
      const nowDt = new Date(baseDate.getTime() + Math.floor(tClamped)*1000);
      sessionNameEl.textContent = fmtHMSfromDate(nowDt);

      // sync followers to master time
      for (const v of followers) {
        const drift = (v.currentTime || 0) - t;
        const adrift = Math.abs(drift);

        if (adrift > HARD_JUMP_TH) {
          v.currentTime = t;
          v.playbackRate = 1.0;
        } else if (adrift > SOFT_SLEW_TH) {
          const sign = (drift > 0) ? -1 : 1; // ahead? slow down; behind? speed up
          const adj = Math.min(MAX_SLEW, Math.max(-MAX_SLEW, sign * Math.min(MAX_SLEW, adrift * 0.5)));
          v.playbackRate = 1.0 + adj;
        } else {
          v.playbackRate = 1.0;
        }
      }

      if (playing && master.requestVideoFrameCallback) {
        rVFCHandle = master.requestVideoFrameCallback(() => loop());
      } else if (playing) {
        rVFCHandle = setTimeout(loop, 33);
      }
    };

    if (master.requestVideoFrameCallback) {
      rVFCHandle = master.requestVideoFrameCallback(() => loop());
    } else {
      rVFCHandle = setTimeout(loop, 33);
    }
  }

  function stopSyncLoop(){
    if (master && master.cancelVideoFrameCallback && rVFCHandle) {
      try { master.cancelVideoFrameCallback(rVFCHandle); } catch {}
    } else if (rVFCHandle) {
      clearTimeout(rVFCHandle);
    }
    rVFCHandle = null;
  }

  // === Detection: overlay helpers ===
  function resizeOverlayToFront(){
    const box = vids.Front.getBoundingClientRect();
    ovlFront.width  = Math.max(1, Math.floor(box.width));
    ovlFront.height = Math.max(1, Math.floor(box.height));
  }
  function clearOverlay(){
    const ctx = ovlFront.getContext('2d');
    ctx.clearRect(0,0,ovlFront.width, ovlFront.height);
  }
  window.addEventListener('resize', resizeOverlayToFront);

  // Ensure TF model is loaded
  async function ensureModelLoaded(){
    if (!cocoModel) {
      // 'lite_mobilenet_v2' is fast; 'mobilenet_v2' is more accurate
      cocoModel = await cocoSsd.load({ base: 'lite_mobilenet_v2' });
    }
  }

  // Map video-intrinsic coordinates to drawn canvas coordinates
  function computeLetterboxMapping(videoEl, canvasEl){
    // video is object-fit: contain; canvas overlays the drawn box.
    const vw = videoEl.videoWidth || 1;
    const vh = videoEl.videoHeight || 1;
    const cw = canvasEl.width || 1;
    const ch = canvasEl.height || 1;
    const videoAspect = vw / vh;
    const canvasAspect = cw / ch;
    let drawW, drawH, padX, padY;
    if (videoAspect > canvasAspect) {
      // limited by width
      drawW = cw;
      drawH = cw / videoAspect;
      padX = 0;
      padY = (ch - drawH) / 2;
    } else {
      // limited by height
      drawH = ch;
      drawW = ch * videoAspect;
      padY = 0;
      padX = (cw - drawW) / 2;
    }
    return { vw, vh, cw, ch, drawW, drawH, padX, padY };
  }

  // Distance estimator using bbox height + camera FOV
  function estimateDistanceMeters(bbox, label){
    // bbox: [x, y, width, height] in *display canvas* pixels
    const Hreal = CLASS_HEIGHTS[label];
    if (!Hreal) return null;
    const imgH = ovlFront.height; // using display height for pixel units
    const imgW = ovlFront.width;
    const hPix = bbox[3];
    if (hPix <= 2) return null;

    const hFovDeg = parseFloat(fovInput.value) || 90;
    const hFov = hFovDeg * Math.PI/180;
    // Convert horizontal FOV to vertical FOV using aspect ratio
    const vFov = 2 * Math.atan( Math.tan(hFov/2) * (imgH/imgW) );
    const fy = (imgH/2) / Math.tan(vFov/2); // pixels

    const Z = (Hreal * fy) / hPix; // meters
    return clamp(Z, 0.1, 300); // keep sane
  }

  function drawDetections(dets){
    const ctx = ovlFront.getContext('2d');
    clearOverlay();

    ctx.lineWidth = 2;
    ctx.font = '12px system-ui, Segoe UI, Roboto, Arial, sans-serif';

    const map = computeLetterboxMapping(vids.Front, ovlFront);

    for (const d of dets){
      const {bbox, class: label, score} = d;
      if (!KEEP.has(label)) continue;
      // coco-ssd returns bbox in *displayed* pixels when run on an HTMLVideoElement,
      // but due to object-fit: contain, we should map into the letterboxed region.
      // bbox = [x, y, width, height] relative to the drawn video rectangle.
      const x = map.padX + bbox[0] * (map.drawW / map.cw);
      const y = map.padY + bbox[1] * (map.drawH / map.ch);
      const w = bbox[2] * (map.drawW / map.cw);
      const h = bbox[3] * (map.drawH / map.ch);

      const dist = estimateDistanceMeters([x,y,w,h], label);
      const labelText = `${label} ${(score*100).toFixed(0)}%` + (dist ? ` • ${round1(dist)} m` : '');

      // Box
      ctx.strokeStyle = 'rgba(72, 190, 255, 0.95)';
      ctx.strokeRect(x, y, w, h);
      // Label bg
      const pad = 4;
      const metrics = ctx.measureText(labelText);
      const textH = 14;
      const textW = metrics.width + pad*2;
      const textX = Math.max(0, Math.min(ovlFront.width - textW, x));
      const textY = Math.max(0, y - (textH + 6));
      ctx.fillStyle = 'rgba(0,0,0,0.65)';
      ctx.fillRect(textX, textY, textW, textH + 4);
      // Text
      ctx.fillStyle = '#eaf6ff';
      ctx.fillText(labelText, textX + pad, textY + textH - 2);
    }
  }

  // Detection loop tied to Front video frames
  function startDetectionLoop(){
    if (detRunning) return;
    detRunning = true;

    const v = vids.Front;
    if (!v || !v.src || !cocoModel) return;

    const step = async () => {
      if (!detRunning || !playing || !detectToggle.checked) return;

      // Simple FPS cap
      const now = performance.now();
      if (now - lastInferT >= INFER_EVERY_MS) {
        lastInferT = now;
        try {
          // Run on the HTMLVideoElement directly; model handles resizing
          const preds = await cocoModel.detect(v, 20); // max 20 boxes
          // Normalize bbox to overlay canvas coordinate system.
          // coco-ssd's bbox for a video element corresponds to rendered pixels of that <video>.
          // We'll remap to our overlay using computeLetterboxMapping proportions.
          drawDetections(preds);
        } catch (e) {
          // If inference fails, keep going but don't spam
          console.error('Detection error:', e);
        }
      }

      if (v.requestVideoFrameCallback) {
        cancelDetCb = v.requestVideoFrameCallback(() => step());
      } else {
        cancelDetCb = setTimeout(step, 33);
      }
    };

    // First run
    if (v.requestVideoFrameCallback) {
      cancelDetCb = v.requestVideoFrameCallback(() => step());
    } else {
      cancelDetCb = setTimeout(step, 33);
    }
  }

  function stopDetectionLoop(clear=false){
    detRunning = false;
    const v = vids.Front;
    if (v && v.cancelVideoFrameCallback && cancelDetCb) {
      try { v.cancelVideoFrameCallback(cancelDetCb); } catch {}
    } else if (cancelDetCb) {
      clearTimeout(cancelDetCb);
    }
    cancelDetCb = null;
    if (clear) clearOverlay();
  }

  pickBtn.onclick = pickFolder;
})();
</script>
</body>
</html>
